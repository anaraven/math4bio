---
title: "Vectors"
date: "2019-08-15 18:57"
category: Matrices
featured: false
tags: [matrix, book, teaching]
links:
  - "[[Matrix/01-optics-intro.Rmd]]"
  - "[[Matrix/02-vectors.Rmd]]"
  - "https://courses.engr.illinois.edu/ece498rc3/fa2016/material/linearAlgebra_4pgs.pdf"
  - "[[Matrix/matrix_drafts/2021/Matrix/matrix_sum]]"
  - "[[Matrix/descriptive-statistics]]"
---

As we said earlier, matrices are a convenient way to write a set of equations

$$
\begin{align}
y_1 & = a_{1,1}\,x_1 + a_{1,2}\,x_2\\
y_2 & = a_{2,1}\,x_1 + a_{2,2}\,x_2
\end{align}
$$

in a simpler way, like

$$
\begin{pmatrix}
y_1 \\ y_2
\end{pmatrix}=
\begin{pmatrix}
a_{1,1} & a_{1,2}\\
a_{2,1} & a_{2,2}
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2
\end{pmatrix}
$$

which can also be written in short as
$$ğ² = ğ€\,ğ±$$

The set of all matrices with real numbers in $m$ rows and $n$ columns is called $â„^{mÃ—n}.$ We write them with bold capital letters, such as $ğ€, ğ, ğ‚, ğ—,$ etc.

The components of the matrix $ğ€$ are single numbers $a_{ij},$ where $i$ indicates the row, and $j$ indicates the column.

Vectors are a particular case of matrices, with only one column. We represent them with bold lower-case letters, such as $ğ±$ or $ğ².$ The set of all vectors with $n$ components is called $â„^n.$

The components of the vector $ğ±$ are single numbers $x_{i},$ where $i$ indicates the row. There is only one column.

We always use italic or greek letters for single numbers. For example, $Î±, Î², Î», a_{ij}, x_i,$ etc.

# Things to do with matrices

We can do many things with matrices, and some of them are useful. Let's see some things we can do.

Something easy and useful is to "turn the matrix sideways". This is called _transposition_. If $ğ€âˆˆ â„^{mÃ— n},$ then we can build a new matrix $ğ€áµ€âˆˆ â„^{nÃ— m}$ such that each component of $ğ€áµ€$ is defined by
$$(Aáµ€)_{ij} = a_{ji}$$
This is called $ğ€$ _transposed_ and is written $ğ€áµ€$ instead of $ğ.$

One practical application of transposition is to write column vectors in the text. Now we can write $ğ›=(x_1,â€¦,x_n)áµ€.$

In general $ğ€â‰ ğ€áµ€.$ If $ğ€$ is square, then sometimes $ğ€=ğ€áµ€,$ in which case we say that $ğ€$ is _symmetrical_.

## Operation on matrices

First, let's compare two matrices in $â„^{2Ã— 2}.$

$$
ğ€=\begin{pmatrix}
1 & 4\\
9 & 16
\end{pmatrix}
\qquad
ğ=\begin{pmatrix}
2 & 8\\
18 & 32
\end{pmatrix}
$$

We can see that each component of $ğ$ is two times the same component of $ğ€.$ In other words, $b_{ij}=2\,a_{ij}$ for all $i$ and all $j.$
This can be written as
$$ğ = 2\,ğ€$$

In general, we can take any matrix $ğ€$ in $â„^{mÃ—n}$, multiply each component by the same constant $Î±âˆˆâ„$, and we get a new matrix $ğ$ in $â„^{mÃ—n}.$ Thus the expression
$$ğ = Î±\,ğ€$$
means that $b_{ij}=Î±\,a_{ij}$ for all $i$ and all $j.$ Each component changes its _scale_, like when we change distance units from meters to inches.

The value $Î±$ is called a _scalar_, and the operation $Î±ğ€$ is called _scalar multiplication_.

Now let's take a look at the following matrices

$$
ğ‚=\begin{pmatrix}
1 & 2\\
3 & 4
\end{pmatrix}
\qquad
ğƒ=\begin{pmatrix}
2 & 6\\
12 & 20
\end{pmatrix}
$$

::: marginnote
We could define a "matrix multiplication" in a similar way, but this is not as useful as the standard matrix multiplication.
:::

If we look carefully, we can see that $d_{ij}=a_{ij}+b_{ij}$ for all $i$ and all $j.$ It is convenient to write this as
$$ğƒ = ğ€ + ğ‚$$
which we call _sum of matrices_.

We can combine sum of matrices with scalar multiplication. If we have several matrices $ğ€, ğ, ğ‚, ğƒâˆˆ â„^{mÃ—n},$ we can combine them like this
$$Î±ğ€ + Î²ğ + Î³ğ‚ + Î´ğƒ$$
and the result will always be a new matrix in $â„^{mÃ— n}.$ This is called a _linear combination_.

In summary, a _linear combination_ can be a regular sum of matrices, or a scalar multiplication, or both at the same time, and the result will always be a matrix. The only condition is that all the matrices must have the same size.

## Matrixâ€“vector multiplication

Let's take a new perspective of the matrix $ğ—$. Each column of $ğ—$ can be seen as a 1â€“column vector:

$$
ğ—=\begin{pmatrix}
x_{1,1} & â‹¯ & x_{1,n}\\
â‹®       & â‹± & â‹®      \\
x_{m,1} & â‹¯ & x_{m,n}\\
\end{pmatrix}
$$

so we can write $ğ—= (ğ±_{(1)} : â€¦ : ğ±_{(n)}).$ Each $ğ±_{(j)}$ is a vector.

Now let's look again at the basic equation that shows how $ğ—$ transforms the vector $ğ›$ into the vector $ğ²$
$$ğ² = ğ— ğ›$$
Since vectors are 1-column matrices, the same rule for matrix multiplication applies. The formula can be written as
$$y_i=\sum_j b_j x_{ij}$$
Now we can write this formula using vectors, as
$$ğ² = \sum_j b_j ğ±_{(j)}$$

In other words, the matrixâ€“vector multiplication can be seen as a linear combination of the columns of $ğ—,$ where the scalars are the components of the vector $ğ›.$

## From _y_ to _b_

Lets assume that we have a vector $ğ²$ and a matrix $ğ—.$ Can we find a vector $ğ›$ such that $ğ² = ğ— ğ›?$

For example, many experiments measure a series of values $y_i$ (let's say, gene expression) that are related to other experimental values $x_{ij}$ (let's say, growth conditions). A typical model for this relation would be
$$y_i = b_1 x_{i,1} + b_2 x_{i,2} + â‹¯ + b_n x_{n,1}$$
for $i=1, â€¦, m.$ This corresponds to the said matrix problem.

The answer will be _"yes"_ if there is a linear combination of the columns of $ğ—$ that corresponds to $y.$

It will be useful to consider the set of all possible linear combinations of a set of vectors. If we have a set of vectors $ğ±_{(1)}, â€¦, ğ±_{(n)},$ we can define the set of all linear combinations of them as

$$
\text{span}(\{ğ±_{(1)}, â€¦, ğ±_{(n)}\})=
\left\{\sum_k Î±_k ğ±_{(k)}: Î±_1, â€¦, Î±_n âˆˆ â„\right\}
$$

When the vectors are the column of a matrix, we can also give a name
$$\text{colspan}(ğ—)= \text{span}(\{ğ±_{(1)}, â€¦, ğ±_{(n)}\})$$
These two sets are essentially the same, but we give two names depending if we work with a matrix or any other set of vectors.

Going back to the original question, we will be able to find $ğ›$ if and only if
$$ğ²âˆˆ\text{colspan}(ğ—)$$

This is sometimes true, sometimes no, as we will see later in detail. If this is true, then we will be able to find $ğ›.$ Unfortunately, in most cases this is not true, so we will consider this case.

# Things we can do with vectors

Since vectors are just a particular case of matrices, everything we can do with matrices, we can do with vectors. But there is more. We can do things with vectors that we cannot do with matrices.

## Dot product

The formula of each equation has the same shape: $y=a_1 x_1 + a_2 x_2.$ This is a way to combine two vectors that will be very useful later. It is called _dot product_.

If $ğš=(a_1,a_2)$ and $ğ›=(b_1,b_2)$ then their dot product is the number
$$ğš â‹… ğ› := a_1 b_1 + a_2 b_2$$
and if we are working on higher dimensions, then
$$ğšâ‹…ğ› := \sum_i a_i b_i$$

It is good to remember that the dot product can be written using matrix multiplication
$$ğšâ‹…ğ› = ğšáµ€ğ›= ğ›áµ€ğš$$

# Geometric interpretation of vectors

## Length of a vector

A vector $ğš=(a_1,a_2)$ corresponds to a point in the plane. The distance between the point $(0,0)$ and $(a_1,a_2)$ is called the _length_ of the vector. We write it as $\Vert ğš \Vert.$

![](../../../images/sirince/img1.svg)

Since ancient times it is known that $a^2+b^2=c^2.$ The sum of the squares is the square of the diagonal.

Using Pythagoras' theorem we can easily see that the length of is

$\Vert ğš\Vert=\sqrt{a_1^2+a_2^2}$

Using the definition of dot product we have $ğšâ‹…ğš=a_1^2+a_2^2$ so we have also $\Vert ğš\Vert= \sqrt{ğšâ‹…ğš},$ which can also be written as
$$\Vert ğš\Vert^2=ğšâ‹…ğš=ğšáµ€ ğš.$$

## Rotating by 90 degrees

If the vector $ğ±$ is $(a,0)$ then it is located on the horizontal axis. If we want to rotate it by 90 degrees, we will get the vector $ğ±'=(0,a).$ Rotating it again we get $ğ±''=(-a,0).$ Easy.

If the vector $ğ²$ is not in the plane, and instead is $(a,b),$ then we can write it as $ğ²= (a,0) + (0,b)$ and each part can rotate separately. The part $(a,0)$ becomes $(0,a),$ and $(0,b)$ becomes $(-b,0).$
Therefore $ğ²'=(-b,a).$ The same reasoning shows that rotating again by 90 degrees will result in $ğ²''=(-a,-b).$

We can draw some conclusions from this exercise. First, changing the signs of each component of a vector will give us a new vector of the same size, but pointing in the opposite direction. This should not be very surprising.

## Perpendicular vectors have dot product zero

The second conclusion is that the dot product of two perpendicular vectors is zero. Indeed we can see that
$$ğ±â‹…ğ±'=(a,0)â‹… (0,a)= 0a + 0a = 0$$
and
$$ğ²â‹…ğ²'=(a, b)â‹… (-b, a)= ab - ab = 0.$$

This result is not only limited to vectors of the same length.
If the vector $ğ³$ is parallel to $ğ²$ but with different length, then $ğ³ = Î± ğ²$ for some number $Î±.$ Then $ğ³$ should be also orthogonal to $ğ².$ Indeed, we have
$$ğ³â‹…ğ²'= (Î±ğ²)â‹…ğ²'= Î±(ğ²â‹… ğ²') = 0.$$

This motivates the following characterization: two vectors $ğ®$ and $ğ¯$ are said to be _orthogonal_ if and only if their dot product is 0. The word _orthogonal_ literally means right angle, and is a fancy way of saying _perpendicular_.

Notice that this implies that the vector $ğŸ$ is always orthogonal to any other vector.

## Orthogonal projection

## Good values and right angles {.no-gap}

![](../../../images/sirince/img2.svg)

- The "smallest" $ğ$ is the one perpendicular to the _plane_ defined by $ğŸ$ and $ğ±$
- In particular
  - The best $ğ$ is perpendicular to $ğŸ$
  - The best $ğ$ is perpendicular to $ğ±$
