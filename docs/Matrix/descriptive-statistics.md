---
title: "Descriptive Statistics"
category: Matrix
date: "2016-08-13 21:36"
mathjax: true
TODO: finish
links:
 - "[[_drafts/2021/Matrix/vectors]]"
 - "[[_drafts/2021/Matrix/matrix_sum]]"
---


The very beginning of statistics is to describe a set of observations. It can be so simple as *counting the number of cases*.

All the basic graphics, such as bar plots, scatter plots, pie graphics and their combinations are also a way to do *descriptive statistics*.

If the observations are numeric, we may ask *"How can we represent this set of numbers with a single number?"*.

Let's say that we have a vector $ğ²=(y_1,\ldots, y_n)$ with $n$ observations, each one with value $y_i$ for $i$ taking values between 1 and $n$. We want to represent this set by a single number $\hat{y}$. We can choose any number to represent it, even numbers that do not belong to this set. For example we can choose $\hat{y}$ to be the smallest value $\min(\{y_i\})$ or the greatest value $\max(\{y_i\})$.

## Which one is better?
To say that a number $\hat{y}$ is better than the rest, we need a way to measure its *goodness*. The criterium most used is to define an **error function** that measures *how much any number fails to represent the complete set of numbers*. For example we may consider the **absolute error**, which can be written as
$$AE(Î²)=\sum_i \vert y_i - Î²\vert,$$
where the vertical bars represent the *absolute value* function.


## It is easier with derivatives
Since all this evaluation is complicated, people traditionally have preferred a different error index. Instead of absolute values, the most common approach uses squared values. They are always positive and have a single minimum that can be found using derivatives. We are talking about the *square error*, which is defined as
$$SE(Î²; ğ²)=\sum_i (y_i-Î²)^2.$$

To find the minimal value we derivate $SE$ with respect to $Î²$ and find the value of $Î²$ that makes the derivative equal to zero.
First, we rewrite the expression for the squared error $SE(Î²; ğ²)$ as
$$SE(Î²; ğ²)= \sum_i y_i^2 - 2Î²\sum_i y_i + n Î²^2.$$

Then we can easily calculate the derivative of $SE()$ with respect to $Î²$, which is
$$\frac{d SE(Î²; ğ²)}{dÎ²}= -2\sum_i y_i + 2 n Î².$$

Making this last formula equal to zero, we have
$$-2\sum_i y_i + 2 n Î²=0$$
and, solving for $Î²,$ we found that the best value to represent $ğ²$ is
$$Î²^* = \sum_i y_i / n,$$

that is, the _mean value_, usually called _average_. 

We usually write the average of $ğ²$ as $\bar{ğ²}.$ Be careful here: the value $\bar{ğ²}$ is a single number, while $ğ²$ is a vector.

## How good is the best one
Now that we have found that $\bar{ğ²}$ is the best value to represent $ğ²$, we want to know *how good* is this representation. In other words, we want to evaluate the square error when $Î²=\bar{ğ²}$, that is
$$SE(\bar{ğ²}; ğ²)= \sum_i y_i^2 - 2\bar{ğ²}\sum_i y_i + n \bar{ğ²}^2.$$
Now we can write $\bar{ğ²}$ as $\sum_i y_i / n$ and we get
$$SE(\bar{ğ²}; ğ²)= \sum_i y_i^2 - \frac{2}{n}\left(\sum_i y_i\right)\left(\sum_i y_i \right) + n \left(\frac{1}{n}\sum_i y_i\right)^2$$
which, after simplification, becomes
$$SE(\bar{ğ²}; ğ²)= \sum_i y_i^2 - \frac{1}{n}\left(\sum_i y_i\right)^2$$

## Minimizing MSE
The error is
$$\mathrm{MSE}(Î², ğ²)=\frac{1}{n}\sum_i (y_i-Î²)^2$$

To find the minimal value we can take the derivative of $MSE$ with respect to $Î²$

$$\frac{d}{d Î²} \mathrm{MSE}(Î², ğ²)= \frac{2}{n}\sum_i (y_i - \beta)$$

> The minimal values of functions are located where the derivative is zero

## Minimizing MSE
Now we find the value of $Î²$ that makes the derivative equal to zero.

$$\frac{d}{d Î²} \mathrm{MSE}(\beta, ğ²)= \frac{2}{n}\sum_i (y_i - \beta)$$

Making this last formula equal to zero and solving for $Î²$ we found that the best one is

$$\beta^* = \frac{1}{n} \sum_i y_i = \bar{ğ²}$$

## Is this a good representative?
If $\bar{ğ²}$ is the best representative, the error is

$$\mathrm{MSE}(\bar{ğ²}, ğ²)=\frac{1}{n}\sum_i (y_i-\bar{ğ²})^2$$

This is called the *variance* of $ğ²$. We write then

$$\mathrm{var}(ğ²)=\frac{1}{n}\sum_i (y_i-\bar{ğ²})^2$$


## Linear model
Now we have a second vector $ğ±$

The new model is
$${y}_i = \beta_0 + \beta_1{x}_i + {e}_i$$
for $i=1,\ldots,n$. All these equations can be written in one as
$$ğ² = Î²_0 ğŸ + Î²_1ğ± + ğ$$

## Mean square error
Now we want to minimize
$$\mathrm{MSE}\left(
\begin{bmatrix}Î²_0\\Î²_1\end{bmatrix}, ğ², ğ±\right)
 = \frac{1}{n}\sum_i (y_i-Î²_0 - Î²_1{x}_i)^2$$
which can also be written as
$$\frac{1}{n}\sum_i e_i^2$$
Indeed, we are minimizing the square of errors (like before)

## Geometrical interpretation {.fl-r}
![](../../../images/sirince/img1.svg)

Since ancient times it is known that $a^2+b^2=c^2$

+ The sum of the squares is the square of the diagonal

Using this idea we see that
$$\sum_i e_i^2 = \mathrm{length}(ğ)^2$$

So we want to minimize the lenght of vector $ğ$.

## Minimizing $ğ$rrors
+ The vectors $\mbfy$, $\mbfx$ and $\mbfe$ have dimension $n$
+ But the vectors $\beta_0\mbfone + \beta_1\mbfx$ have only 2 degrees of freedom
+ All these vectors lie in a *plane* of dimension 2

We want to find the "good" $Î²_0$ and $Î²_1$ that minimize  $\Vert ğ\Vert$

## Good values and right angles
![](../../../images/sirince/img2.svg)

+ The "smallest" $mbfe$ is the one perpendicular to the *plane* defined by $mbfone$ and $mbfx$
+ In particular
    + The best $mbfe$ is perpendicular to $mbfone$
    + The best $mbfe$ is perpendicular to $mbfx$

## Dot product|linear algebra to the rescue
Ancient knowledge again:

The vector $ğ$ is perpendicular to $ğ±$ if and only if
$$ğ±^Tğ=0$$

In the same way, $ğ$ is perpendicular to $ğŸ$ if
$$ğŸ^Tğ=0$$

## Seeing the big picture
We can see the big picture if we use matrices:
$$\begin{pmatrix}
1 & x_1\\
â‹® & â‹® \\
1 & x_n
\end{pmatrix}= \begin{pmatrix}ğŸ & ğ±\end{pmatrix}=ğ€$$
$$\begin{pmatrix}Î²_0\\ Î²_1\end{pmatrix}=ğ›$$
then the smallest $ğ$ obeys
$$ğ€^T ğ = 0$$

## Finding beta
The model was
$$ğ² = ğ€ ğ› + ğ$$
so the error is
$$ğ = ğ² - ğ€ ğ›$$
Multiplying by $ğ€^T$ we have
$$ğ€^T ğ = ğ€^T ğ² - ğ€^T ğ€ ğ›$$

## Finding beta
To have $ğ€^T ğ = 0$ we need to make
$$ğ€^T ğ² = ğ€^T ğ€ ğ›^*$$
We write $ğ›^*$ because these are the "good"
$Î²_0^*$ and$Î²_1^*$

Now, if $ğ€^T ğ€$ is "well behaved",
$$ğ›^* = (ğ€^T ğ€)^{-1}ğ€^T ğ²$$

----

## Mean Square Error
Replacing $ğ›^* = (ğ€^T ğ€)^{-1}ğ€^T ğ²$ in the formula of error
$$ğ = ğ² - ğ€ ğ›$$
we have
$$ğ^* = (ğˆ - ğ€(ğ€^T ğ€)^{-1}ğ€^T )ğ²$$
(no surprise, simple substitution)

What happens to the mean square error $\mathrm{MSE}(ğ›^*,ğ², ğ±)=\frac{1}{n}\sum_i e_i^2$?

## Mean Square Error

$$\begin{aligned}
\mathrm{MSE}(ğ›^*,ğ², ğ±)
& =\frac{1}{n}\sum_i e_i^2=\frac{1}{n}ğ^Tğ\\
& =\frac{1}{n}ğ²^T (ğˆ-ğ€(ğ€^Tğ€)^{-1}ğ€^T)^T(ğˆ-ğ€(ğ€^Tğ€)^{-1}ğ€^T) ğ²\\
& =\frac{1}{n}ğ²^T (ğˆ - ğ€(ğ€^T ğ€)^{-1}ğ€^T) ğ²
\end{aligned}$$
(do the algebra and see that many things vanish)

So the *Mean Square Error* depends on $ğ²$ and $ğ€$, which depends on $ğ±$. Choose them carefully

----

## Square Error
Replacing $ğ›^* = (ğ€^T ğ€)^{-1}ğ€^T ğ²$ in the formula of error
$$ğ = ğ² - ğ€ ğ›$$
we have
$$ğ^* = (ğˆ - ğ€(ğ€^T ğ€)^{-1}ğ€^T )ğ²$$
(no surprise, simple substitution)

What happens to the mean square error $\mathrm{SE}(ğ›^*;ğ², ğ±)=\sum_i e_i^2$?

## Square Error

$$\begin{aligned}
\mathrm{SE}(ğ›^*, ğ², ğ±)
& =\sum_i e_i^2=ğ^Tğ\\
& =ğ²^T (ğˆ - ğ€(ğ€^T ğ€)^{-1}ğ€^T)^T(ğˆ - ğ€(ğ€^T ğ€)^{-1}ğ€^T) ğ²\\
& =ğ²^T (ğˆ - ğ€(ğ€^T ğ€)^{-1}ğ€^T) ğ²
\end{aligned}$$
(do the algebra and see that many things vanish)

So the *Mean Square Error* depends on $ğ²$ and $ğ€$, which depends on $ğ±$. Choose them carefully.

----

## Generalization
All the argument is valid if $ğ€$ has any number of columns

+ that is, any number of independent variables
+ at least 1
+ at most $n$

## The case of one variable
(result should be the mean)

If there is no independent variable (i.e. there is no $x$), we have $ğ€ =ğŸ,$ then
$$\begin{aligned}
ğ›^* & = (ğ€^T ğ€)^{-1}ğ€^T ğ²\\
ğ›^* & = (ğŸ^T ğŸ)^{-1}ğŸ^T ğ²\\
ğ›^* & = \frac{1}{n}\sum{y}_i
\end{aligned}$$
just as before.

## One variable case
mean square error
$$\begin{aligned}
\frac{1}{n}ğ^Tğ 
& = \frac{1}{n}ğ²^T (ğˆ - ğ€(ğ€^T ğ€)^{-1}ğ€^T) ğ²\\
& = \frac{1}{n}ğ²^T (ğˆ - ğŸ(ğŸ^T ğŸ)^{-1}ğŸ^T) ğ²\\
& = \frac{1}{n}ğ²^T ğ² - \frac{1}{n}ğ²^TğŸ(n)^{-1}ğŸ^T ğ²\\
& =\frac{1}{n}\sum{y}_i^2 - (\frac{1}{n}\sum{y}_i)^2
\end{aligned}$$
just as before.

## One variable case: square error
$$\begin{aligned}
ğ^Tğ & = ğ²^T (ğˆ - ğ€(ğ€^T ğ€)^{-1}ğ€^T) ğ²\\
    & = ğ²^T (ğˆ - ğŸ(ğŸ^T ğŸ)^{-1}ğŸ^T) ğ²\\
    & = ğ²^T ğ² - \frac{1}{n}ğ²^TğŸ(n)^{-1}ğŸ^T ğ²\\
    & = \sum{y}_i^2 - (\frac{1}{n}\sum{y}_i)^2
\end{aligned}$$
just as before

## "Good behavior"
The only condition to have a solution is
$$ğ€^T ğ€$$ has an inverse. This is equivalent to

> All columns of $ğ€$ are *linearly independent*

## Linear independence

## Designing experiments
