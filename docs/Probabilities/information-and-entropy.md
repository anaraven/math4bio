---
title: "Information"
date: "2021-04-18 12:15"
original_date:
  - "2019-02-27 13:26"
  - "2019-03-31 22:43"
featured: false
mathjax: true
bibliography: '/Users/anaraven/Web/blog/_priv/bib/library.bib'
output:
    tufte::tufte_handout: default
nocite: |
  @Martin2013a
---

We want to measure or quantify the information we learn from a message or an experiment.

## How many bits
Suppose we receive a message, made up of a series of symbols. Each symbol belongs to an alphabet of size $M$. We can also assume that we have a device that, each time we use it, randomly produces one of $M$ possibilities. For this analysis it does not matter if the message is generated by a person, by a machine, or if the person sending the message uses a device to generate it. The device generates a message, the sender knows the message, we receive the message

To represent in binary code any of the symbols of an alphabet with $M$ symbols, $\log_2 M$ bits are needed. For example, if each symbol can take only one of two values ​​(that is, if $M = 2),$ then each binary symbol requires one bit. Two symbols require two bits, and N binary symbols require N bits [^bits].

[^bits]: In general, if we represent each symbol with a code in base $B,$ we will need $\log_B M$ digits. Changing the base is equivalent to changing units, something like changing from meters to feet. The units when we use logarithms in base 2 are *bits*, for natural logarithms it is *nats*.

The number of bits of each $x$ data is $B(x) = \log M,$ where $M$ is the number of possible symbols ^[In this part all logarithms are in base 2, unless otherwise indicated contrary. That is, $$\log(x) = \log_2 (x)$$].

The number of bits is additive. If the message has two symbols, we will have double bits [^double]
$$B (x_1, x_2) = B (x_1) + B (x_2) = \log(M) + \log(M)$$
and if it has $n$ symbols we have $$B (x_1, …, x_n) = n \log(M) + \log(M)$$

[^double]: This can also be seen as if the pair $(x_1, x_2)$ were a symbol of a new alphabet with $M^2$ symbols 
$$B (x_1, x_2) = 2 \log(M) = \log(M ^ 2)$$

If each symbol $x_i$ is taken from different alphabets, each one of size $M_i$, then each symbol contributes a different number of bits
$$B (x_1, x_2) = B (x_1) + B (x_2) = \log(M_1) + \log(M_2).$$

Of course this argument is also valid for $N> 2$ symbols.
$$B(x_1, …, x_N) = \sum_{i = 1}^N B(x_i) = \sum_{i = 1}^N \log(M_i).$$

Generalizing, we can write
$$I (x, y) = I (x) + I (y) \text {when they are independent.}$$
 
In a first approximation we say that the *amount of information* of a symbol is $\log N$ bits. That is, assuming that all symbols are equally likely. That is, all symbols have probability $p = 1/N$.

$$\log N = - \log(1/N) = - \log p = - \sum p \log p $$

## Data versus Information
In the previous section we described the size of the data, regardless of the _context_ in which the message is received. Now we are going to distinguish between *data* and *information*. In particular we are going to call *information* the part of the data that tells us something we did not know [^info].

[^info]: It is often said that *information* is the part of the data that allows you to make a decision.

For example, if we know *a priori* that we will receive two identical symbols [^1], then only the first symbol gives us positive information. The second symbol does not tell us anything new, that is, it contributes zero bits.

To represent the amount of information in a message $m$ and its dependence on the context $C$, we write $I (m | C).$ If we receive two symbols but we know a priori that the second is equal to the first, then
$$I (x_1, x_2 | x_2 = x_1) = B (x_1) + 0$$

At the other extreme we have the case where each symbol in the message is independent, so that each symbol tells us something new. Knowing $x_1$ tells us nothing about $x_2.$ In this case, each symbol $x_i$ contributes $B (x_i)$ bits:
$$I(x_1, x_2 | x_2 \textrm{ independent of } x_1) = B (x_1) + B (x_2)$$


These two cases are instances of the general case: when the second symbol can only take certain values ​​that depend on the first symbol. For example, it may be that a consonant can only be followed by a vowel. In Spanish the letter *q* is always followed by the letter *u*.

In the general case, the value of $x_1$ will indicate which part of the alphabet is used for $x_2$. In that case the number of possible symbols for $x_2$ is $M_2(x_1),$ an amount that depends on the symbol $x_1.$ That is, the number of possibilities for $x_2$ depends on the value of $x_1.$
$$I (x_1, x_2 | x_2 \textrm{ depends on } x_1) = \log(M_1) + \log(M_2 (x_1))$$
<! - = I (x_1) + I (x_2 | x_1) ->

We can see that this expression recovers the two previous particular cases. When we know that $x_2 = x_1$ then the alphabet for $x_2$ is reduced to $\{x_1 \},$ that is $M_2 (x_1) = 1.$ Then we will have
$$I(x_1, x_2 | x_2 = x_1) = \log(M) + \log(M_2 (x_1)) = \log(M) + 0$$
When each symbol of the message is independent, then $M_2 (x_1) = M$ independent of $x_1,$ and we will have

$$\begin{aligned}
I (x_1, x_2 \vert x_2 \text { independent of } x_1) & = \log(M_1) + \log(M_2 (x_1)) \\
& = \log(M) + \log(M)
\end {aligned}
$$

<! - The reasoning is the same if the symbols are transmitted in another order. Then the information of the pair $(x, y)$ must be symmetric, that is, $I (x, y) = I (y, x).$ ->

[^ 1]: We can think that this redundancy is used to ensure the transmission of the message even when a symbol is lost. In that case it is also possible to recover the first symbol from the second.

The general formula for the information provided by a message $m = (x_1, x_2)$ in a $C$ context is:
$$I(x_1, x_2 | C) = I (x_1 | C) + I (x_2 | x_1, C)$$

Note that if, given the context $C$, knowing the value of $x_1$ does not tell us anything about $x_2$, then $I(x_2 | x_1, C) = I (x_2 | C)$. This is what we call *independence* of $x_1$ and $x_2$ in the $C$ context.

To simplify the notation, the $C$ context is usually omitted. This is not a problem, as long as we remember that *there is always a context,* and that the independence between two symbols depends on the context. In this case we write
$$I (x_1, x_2) = I (x_1) + I (x_2 \vert x_1)$$


## When symbols have different probability
We continue with the idea of ​​an alphabet $\mathcal A$ with $M$ symbols, all equally likely.
Suppose that the alphabet can be partitioned into several disjoint subsets [^disj]. For example the alphabet can be broken down into vowels, consonants, and digits. Each subset can be of a different size. For example, in English there are 5 vowels, 21 consonants, and 10 digits.

[^ disj]: This means that the alphabet $\mathcal A$ has several subsets $S_i$ such that the intersection between two different subsets is empty ($i≠j ⇒ S_i ∩ S_j = ϕ$) and the union of all subsets is equal to the entire alphabet $(\mathcal A = ∪_i S_i)$.

Suppose we receive a symbol $x$ that happens to belong to a subset $\mathcal G ⊂ \mathcal A$ that contains $M_{\mathcal G}$ elements. The probability that any symbol is in the subset $\mathcal G$ is $M_\mathcal G / M$.

The information given by the $x$ symbol can be broken down into two parts

+ The information of "we receive a symbol from the class $\mathcal G$", and
+ the information of "it is specifically the symbol $x ∈ \mathcal G$".

That is, the information given by $x$ is broken down into two parts
$$I(x) = I(\mathcal G) + I(x | \mathcal G)$$

Then we can express the information in $\mathcal G$ as
$$I (\mathcal G) = I (x) - I (x \vert \mathcal G) = \log M - \log M_ \mathcal G = - \log\frac {M_ \mathcal G} {M}$$
That is, the information given by the class $\mathcal G$ is the logarithm of the probability of the class $\mathcal G$, with a negative sign.

For example, the symbol $e$ decomposes into _is a vowel_, and _is the second vowel_. The information for the _"vowel"_ symbol is
$$I(\textrm{vowel}) = I(e) - I (e | \text {vowel}) = \log(26) - \log(5) = - \log\left (\frac{5}{26} \right)$$

Of course, partitioning the alphabet into classes of different sizes is the same as defining symbols of different probability. That is, we have a new alphabet $\mathcal A'$ in which each letter represents a class of symbols in the original alphabet. The difference is that, in $\mathcal A'$, the symbols have different probabilities.

In short, the information a symbol gives us depends on the probability we assign to it. The information of any symbol $x$, given the context $C,$ is
$$I (x | C) = - \log\Pr (x | C).$$
In particular the conditional information, when we already know the first symbol, is
$$I(x_2 \vert x_1) = - \log\Pr (x_2 \vert x_1)$$
well

$$
\begin {aligned}
I (x_2 \vert x_1) & = I (x_1, x_2) - I (x_1) \\
 & = - \log\Pr (x_1, x_2) + \log\Pr (x_2) \\
 & = - \log\frac {\Pr (x_1, x_2)} {\Pr (x_2)} \\
 & = - \log\Pr (x_2 \vert x_1)
\end {aligned} $$

Since the information depends on the logarithm of the probability, we have another interesting result. The probability product rule says:
$$\Pr (x_1, x_2) = \Pr (x_1) \Pr (x_2 \vert x_1)$$
Then, taking logarithms, we will have
$$\log(\Pr (x_1, x_2)) = \log(\Pr (x_1)) + \log(\Pr (x_2 \vert x_1))$$
where do we get
$$I(x_1, x_2) = I (x_1) + I (x_2 \vert x_1).$$
If all symbols are independent, then
$$I(x_1, x_2, …) = I (x_1) + I (x_2) + ⋯$$

In the particular case in which all symbols have the same probability $ p = 1 / M $, then the information is
$$I (x) = - \log p = \log M$$
just as we defined before. If all the symbols are also independent, then the amount of information is equal to the number of data bits. All data has the same amount of information

If we receive a symbol that has probability 1, then the information is 0. If we receive a symbol with probability 0, then the information is $ \infty $. This justifies the other name for the _information_: it is the _surprise_.
 
# Average information
The information for each symbol depends on the probability of the symbol.
The average amount of information is
$$H = -\sum p \log p.$$

Wikipedia says "as a consequence of Shannon's source coding theorem, the entropy is a measure of the smallest codeword length that is theoretically possible" for a given alphabet with associated probabilities.

If there are some symbols more likely than others, it can be efficient to use short messages for more frequent symbols and longer messages for less frequent symbols. The best thing is that each symbol with probability $p$ uses $(-\log p)$ bits. This is the Huffman compression idea.