---
title: "Simple linear model"
date: "2021-10-02 14:32:55"
thanks: "October 8, 2021"
author: AndrÃ©s Aravena, Ph.D.
institute: Istanbul University
lang: en
documentclass: IEEEtran
fontsize: 10pt
classoption: a4paper
featured: false
mathjax: true
links:
  - "[[_drafts/2021/math4bio/sample-average-variance.md]]"
  - "[[_drafts/2021/math4bio/vector-mean-variance.md]]"
---

We have again two vectors $ğ±$ and $ğ²$ with the same length $n,$ and we want to explore their relationship. In particular, we want to see if there is a straight line connecting them. If that was the case, there are two constants (let's call them $Î²_0$ and $Î²_1$) that would allow us to write an expression like
$$y_i â‰Ÿ Î²_0+Î²_1 x_i$$
It turns out that, in many relevant cases, this equation is false but no too much. For each value of $i$ there will be a difference
$Î²_0+Î²_1 x_i - y_i$
that we call the _residual_.

The total error of approximating {$y_i$} with a straight line is given by the sum of all residuals
$$L(Î²_0, Î²_1)=\sum_i (Î²_0+Î²_1 x_i - y_i)^2$$
We write it as a function $L(Î²_0, Î²_1)$ to emphasize that the total error depends only on $Î²_0$ and $Î²_1$ since the values $\{x_i\}$ and $\{y_i\}$ are fixed. They are the data, that is, they are given to us.

We want the smallest possible error. We will look for the "best" values of $Î²_0$ and $Î²_1,$ that is, the ones that make $L(Î²_0, Î²_1)$ smallest. The easiest way to find the best values is to take derivatives. We do not need to use a lot of calculus here. We only need to know that, in every point where $L$ has a minimum, its derivative must be 0. Moreover, it can be shown that this particular expression has one and only one point where the derivative is 0.

Since we have two variables, we take two partial derivatives. We want to solve the system
$$\frac{âˆ‚L(Î²_0, Î²_1)}{âˆ‚Î²_0}=0,\qquad \frac{âˆ‚L(Î²_0, Î²_1)}{âˆ‚Î²_1}=0$$

The first expression can be written as
$$\begin{aligned}
\frac{âˆ‚L(Î²_0, Î²_1)}{âˆ‚Î²_0} &=\sum_i 2(Î²_0+Î²_1 x_i - y_i)\\
&= 2 n Î²_0 + 2Î²_1\sum_i x_i - 2\sum_i y_i
\end{aligned}$$

We are looking for conditions when this expression is equal to zero, thus
$$2 n Î²_0 + 2Î²_1\sum_i x_i - 2\sum_i y_i=0$$
which can be rewritten as
$$Î²_0 + \frac{1}{n}Î²_1\sum_i x_i = \frac{1}{n}\sum_i y_i$$
in other words
$$\overline{ğ²} = Î²_0 + Î²_1 \overline{ğ±}$$

This is nice. The point $(\overline{ğ±}, \overline{ğ²})$ satisfies the line equation. It means that the best straight line will always pass through both averages at the same time. 

For the second condition, the derivative with respect to $Î²_1$ is
$$\begin{aligned}\frac{âˆ‚L(Î²_0, Î²_1)}{âˆ‚Î²_0}
 &=\sum_i 2(Î²_0+Î²_1 x_i - y_i)â‹…x_i\\
 &=2 Î²_0 \sum_i x_i + 2 Î²_1 \sum_i x_i^2 -2\sum_i x_i y_i
\end{aligned}$$
This expression must be equal to 0. That is
$$Î²_0 \sum_i x_i + Î²_1 \sum_i x_i^2 = \sum_i x_i y_i$$

Since we know that $Î²_0 = \overline{ğ²} - Î²_1 \overline{ğ±}$ we can replace it in the previous expression, and we get
$$(\overline{ğ²} - Î²_1 \overline{ğ±}) \sum_i x_i + Î²_1 \sum_i x_i^2 = \sum_i x_i y_i$$
that is
$$\overline{ğ²}\sum_i x_i - Î²_1 \overline{ğ±} \sum_i x_i + Î²_1 \sum_i x_i^2 = \sum_i x_i y_i$$

Now we remember that $âˆ‘ x_i = n\overline{ğ±},$ so we can rewrite this last expression as
$$n\overline{ğ±}\overline{ğ²} - Î²_1 n\overline{ğ±}^2 + Î²_1 \sum_i x_i^2 = \sum_i x_i y_i$$
then, dividing by $n,$ we have
$$Î²_1 \left(\frac{1}{n}\sum_i x_i^2 - \overline{ğ±}^2\right) = \frac{1}{n}\sum_i x_i y_i - \overline{ğ±}\overline{ğ²}$$

You may recognize that
$$\frac{1}{n}\sum_i x_i^2 - \overline{ğ±}^2=\text{var}(ğ±)$$
and
$$\frac{1}{n}\sum_i x_i y_i - \overline{ğ±}\overline{ğ²}=\text{cov}(ğ±,ğ²)$$

Finally, we get
$$\begin{aligned}
Î²_1 &= \frac{\text{cov}(ğ±,ğ²)}{\text{var}(ğ±)}\\
Î²_0 &= \overline{ğ²} - Î²_1 \overline{ğ±}
\end{aligned}$$

# How bad is this model
We can measure the quality of a model using the same philosophy as before, with the average of the square of errors. This number is called _mean square error_.

The simplest model of $ğ²$ is just $\overline{ğ²}$. In that case, the mean square error is
$$MSE_0=\frac{1}{n}\sum_i (y_i - \overline{ğ²})^2$$

For a simple linear model we have
$$MSE_1=\frac{1}{n}\sum_i (y_i - Î²_0 - Î²_1 x_i)^2$$

Replacing $Î²_0 = \overline{ğ²} - Î²_1 \overline{ğ±}$ we have
$$MSE_1=\frac{1}{n}\sum_i (y_i - \overline{ğ²} + Î²_1 \overline{ğ±} - Î²_1 x_i)^2$$
which we can rewrite as
$$MSE_1=\frac{1}{n}\sum_i \left((y_i - \overline{ğ²}) - Î²_1(x_i - \overline{ğ±})\right)^2$$

Expanding the binomial square we get
$$MSE_1=\frac{1}{n}\sum_i (y_i - \overline{ğ²})^2 - 2Î²_1(y_i - \overline{ğ²})(x_i - \overline{ğ±}) + Î²_1^2(x_i - \overline{ğ±})^2$$
which can be rewritten as
$$MSE_1=\text{var}(ğ²) - 2Î²_1\text{cov}(ğ±,ğ²) + Î²_1^2\text{var}(ğ±).$$

Now we replace $Î²_1$ by its calculated value
$$MSE_1=\text{var}(ğ²) - 2\frac{\text{cov}(ğ±,ğ²)}{\text{var}(ğ±)}\text{cov}(ğ±,ğ²) + \frac{\text{cov}^2(ğ±,ğ²)}{\text{var}^2(ğ±)}\text{var}(ğ±)$$
which simplifies to
$$MSE_1=\text{var}(ğ²) - \frac{\text{cov}^2(ğ±,ğ²)}{\text{var}(ğ±)}.$$

This already shows that the mean square error is better in model 1.

# Relative improvement and Correlation
The variance in the new model is better, but how much?
The absolute change in variance is 
$$MSE_0 - MSE_1=\frac{\text{cov}^2(ğ±,ğ²)}{\text{var}(ğ±)}.$$
and the relative change is
$$\frac{MSE_0-MSE_1}{MSE_0}=\frac{\text{cov}^2(ğ±,ğ²)}{\text{var}(ğ±)\text{var}(ğ²)}.$$

This number represents the percentage of variance that is explained by the model.
The name of this number is $R^2$.

You may remember that the _Pearson correlation coefficient_ between two variables is
$$r=\frac{\text{cov}(ğ±,ğ²)}{\text{sd}(ğ±)\text{sd}(ğ²)}$$
so we have **in this case** that 
$$R^2 = r^2$$

This is valid for linear models with a single independent variable.
It will not be valid for models with several predictors.
