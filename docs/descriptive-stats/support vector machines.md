---
title: "Suppor vector machines"
date: "2021-05-08 19:25:36"
lang: es
featured: false
---

Sea I={1â€¦N} el conjunto de individuos, caracterizados por el conjunto de vectors X={ğ±_i: iâˆˆN}. Los individuos se separan en dos categorÃ­as: aprobados y rechazados.

Es decir, el conjunto I se puede escribir como I=AâˆªB, donde A es el subconjunto de individuos aprobados y B contiene los individuos rechazados. Supongamos en primera instancia que A y B se pueden separar perfectamente por un hiperplano ğ»(ğ°,b), definido por la ecuaciÃ³n ğ°áµ€ğ±+b=0, de modo que  
    ğ°áµ€ğ±_i+b>0   for all iâˆˆ A  
    ğ°áµ€ğ±_i+b<0   for all iâˆˆ B

Notemos que este hiperplano no cambia si ğ° y b se multiplican por un escalar Î±>0. Es decir ğ»(ğ°,b)=ğ»(Î±ğ°,Î±b) para cualquier Î±>0. Usaremos esta propiedad mÃ¡s adelante.

Obviamente puede haber muchas combinaciones de ğ° y b que definan hiperplanos que separan ambos conjuntos. Nos gustarÃ­a encontrar "el mejor" hiperplano.

En un problema de clasificaciÃ³n nos interesa que el clasificador sea robusto, es decir que la regla de clasificaciÃ³n sea insensible a perturbaciones pequeÃ±as. Para asegurar esto, buscamos un hiperplano que estÃ© lo mÃ¡s lejano posible del conjunto A y del conjunto B, simultÃ¡neamente.

Para encontrar "el mejor hiperplano" necesitamos entonces medir la distancia del hiperplano al conjunto A. Sea ğ±_AâˆˆA el vector en A mÃ¡s cercano al hiperplano ğ»(ğ°,b). 

<!-- hay que definir cÃ³mo se mide la distancia punto-hiperplano -->

Para simplificar la bÃºsqueda, vamos a reducir los grados de libertad imponiendo la condiciÃ³n  
    ğ°áµ€ğ±_A+b=1

Del mismo modo, si ğ±_B es el punto en B mÃ¡s cercano a H(w,b), imponemos ğ°áµ€ğ±_B+b= -1


## Hiperplanos
Dado un vector $wâˆˆâ„^d$ fijo, el conjunto H(w,0)={x: xâˆˆâ„^d, w^Tx=0} estÃ¡ formado por todos los vectores que son ortogonales a w. Es un subespacio de dimensiÃ³n d-1, que corresponde a un hiperplanto que pasa por el origen.

El hiperplano que pasa por x_0 es H(w,0)+x_0 = {x+x_0: xâˆˆâ„^d, w^Tx=0}
En otras palabras, tenemos w^T(x-x_0)=0, es decir w^Tx - w^Tx_0=0.
En resumen, $H(w,0)+x_0 = H(w,b)$ donde $b=-W^Tx_0$

En general H(w, b)={x: xâˆˆâ„^d, w^Tx-b=0} es un hiperplano ortogonal a w que pasa por un punto x_0â‰ 0. Es decir, si bâ‰ 0, entonces H(w, b) no pasa por el origen.

Si nos movemos en la direcciÃ³n de w, el valor de $w^Tx-b$ aumenta.

Se pueden escoger w y b tales que $w^Tx_A-b=1$ y $w^Tx_B-b=-1$. Es facil ver, en ese caso, que 

$$\begin{aligned}
w^Tx_i-b &â‰¥ +1\qquad âˆ€iâˆˆA\\
w^Tx_A-b &â‰¤ -1\qquad âˆ€iâˆˆB
\end{aligned}$$

## Etiquetas
Sea $y_i=1$ para todos los individuos $iâˆˆA$, y sea $y_i=-1 âˆ€iâˆˆB$
<!-- $$y_i=[iâˆˆA]-[iâˆˆB]$$ -->

$$y_i =\begin{cases}
 +1\qquad âˆ€iâˆˆA\\
 -1\qquad âˆ€iâˆˆB
\end{cases}$$

Entonces tendremos siempre que
$$y_i(w^Tx_i-b) â‰¥ 1 \qquad âˆ€iâˆˆI$$

## distancia
Sabemos como calcular la distancia entre dos puntos. Escribimos
$$\Vert x_i-x_j\Vert=\sqrt{\sum_k (x_{ik}-x_{jk})^2}$$

La distancia entre un punto $x_i$ y el plano $(w^Tx-b)=0$ es la distancia entre $x_i$ y su sombra $Px_i$

$$w^Tx_A-b=1$$
$$w^T Px_A-b=0$$
Therefore
$$w^T(x - Px_A)=1$$
which means that
$$\Vert w\Vert \Vert x_i-Px_i\Vert\cosÎ¸=1$$
where Î¸ is the angle between $w$ and $(x_i-Px_i)$. Since the projection is orthogonal, 
sabemos que ambos vectores son paralelos, es decir Î¸=0. Concluimos que
$$\Vert x_i-Px_i\Vert = \frac{1}{\Vert w\Vert}$$

## Buscando ğ‘¤
El ancho de la brecha es $1/\Vert w\Vert$. Hay que maximizar/minimizar $\Vert w\Vert,$ o equivalentemente $w^Tw/2$, sujeto a $y_i(w^T x_i + b)â‰¥1$

El Lagrangiano es
$$â„’=\frac{w^Tw}{2} + \sum_i Î»_i(y_i(w^T x_i + b)-1)$$
cuyas derivadas son 0 cuando encontramos el Ã³ptimo.
$$\frac{âˆ‚â„’}{âˆ‚w}=w + \sum_i Î»_i y_i x_i=0$$
$Î»_iâ‰¥0$ y $Î»_i(y_i(w^T x_i + b)-1)=0 \quad âˆ€i$